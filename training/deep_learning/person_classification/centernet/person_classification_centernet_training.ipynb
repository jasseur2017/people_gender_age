{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import albumentations as A\n",
    "import albumentations.augmentations.geometric.functional as F\n",
    "import cv2\n",
    "\n",
    "class AspectRatioResize(A.DualTransform):\n",
    "    \"\"\"Resize the input to the given height and width.\n",
    "\n",
    "    Args:\n",
    "        p (float): probability of applying the transform. Default: 1.\n",
    "        height (int): desired height of the output.\n",
    "        width (int): desired width of the output.\n",
    "        interpolation (OpenCV flag): flag that is used to specify the interpolation algorithm. Should be one of:\n",
    "            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n",
    "            Default: cv2.INTER_LINEAR.\n",
    "\n",
    "    Targets:\n",
    "        image, mask, bboxes\n",
    "\n",
    "    Image types:\n",
    "        uint8, float32\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, height, width, scale=(1, 1), interpolation=cv2.INTER_LINEAR, always_apply=False, p=1\n",
    "    ):\n",
    "        super(AspectRatioResize, self).__init__(always_apply, p)\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.scale = scale\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def apply(self, img, interpolation=cv2.INTER_LINEAR, **params):\n",
    "        height, width, _ = img.shape\n",
    "        r = min(self.width / width, self.height / height)\n",
    "        scale = random.uniform(*self.scale)\n",
    "        return F.resize(\n",
    "            img, height=int(scale * r * height), width=int(scale * r * width),\n",
    "            interpolation=interpolation\n",
    "        )\n",
    "\n",
    "    def apply_to_bbox(self, bbox, **params):\n",
    "        # Bounding box coordinates are scale invariant\n",
    "        return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "# IMAGE_SIZE = (416, 416)\n",
    "IMAGE_SIZE = (1088, 608)\n",
    "# IMAGE_SIZE = (608, 608)\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        AspectRatioResize(height=IMAGE_SIZE[1], width=IMAGE_SIZE[0], scale=(0.2, 1.0), p=1.0),\n",
    "        A.PadIfNeeded(\n",
    "            min_height=IMAGE_SIZE[1], min_width=IMAGE_SIZE[0], border_mode=cv2.BORDER_CONSTANT, #BORDER_WRAP,\n",
    "            value=[128, 128, 128]\n",
    "        ),\n",
    "#         A.Resize(height=IMAGE_SIZE[1], width=IMAGE_SIZE[0], p=1.0)\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "    ], \n",
    "    p=1.0, \n",
    "    bbox_params=A.BboxParams(\n",
    "        format=\"pascal_voc\",\n",
    "        min_area=100,\n",
    "        min_visibility=0,\n",
    "        label_fields=[\"labels\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "val_transform = A.Compose(\n",
    "    [\n",
    "        AspectRatioResize(height=IMAGE_SIZE[1], width=IMAGE_SIZE[0], scale=(1.0, 1.0), p=1.0),\n",
    "        A.PadIfNeeded(\n",
    "            min_height=IMAGE_SIZE[1], min_width=IMAGE_SIZE[0], border_mode=cv2.BORDER_CONSTANT,\n",
    "            value=[128, 128, 128]\n",
    "        )\n",
    "#         A.Resize(height=IMAGE_SIZE[1], width=IMAGE_SIZE[0], p=1.0)\n",
    "    ],\n",
    "    p=1.0,\n",
    "    bbox_params=A.BboxParams(\n",
    "        format=\"pascal_voc\",\n",
    "        min_area=0,\n",
    "        min_visibility=0,\n",
    "        label_fields=[\"labels\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENDERS = [\"M\", \"F\"]\n",
    "AGES = [\"1-2\", \"3-9\", \"10-20\", \"21-25\", \"26-27\", \"28-31\", \"32-36\", \"37-45\", \"46-54\", \"55-65\", \"66-116\"]\n",
    "LABELS = GENDERS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crowd human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_crowd_dir = Path(\"/home/username/Downloads/crowdhuman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_df(data_file):\n",
    "    data_df = pd.read_csv(data_file)\n",
    "    # data_df = data_df[(data_df[\"head_attr.ignore\"] != 1) & (data_df[\"head_attr.occ\"] != 1)]\n",
    "    data_df = data_df.rename(columns={\"gender\": \"label\"})\n",
    "    # data_df = data_df.rename(columns={\"age\": \"label\"})\n",
    "    data_df = data_df[data_df[\"label\"].notnull()] # & data_df[\"age\"].notnull()]\n",
    "    data_df = data_df.reset_index(drop=True)\n",
    "    data_df.loc[pd.notnull(data_df[\"label\"]), \"label\"] = (\n",
    "        data_df.loc[pd.notnull(data_df[\"label\"]), \"label\"].apply(lambda x: LABELS.index(x))\n",
    "        )\n",
    "    \n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# train_crowd_file = Path(\"../../../mlops/crowdhuman/annotation_train_with_classes.csv\")\n",
    "train_crowd_file = Path(\"../../../mlops/crowdhuman/correction/checkpoint_annotations.csv\")\n",
    "train_crowd_df = read_df(train_crowd_file)\n",
    "train_crowd_df = train_crowd_df.rename(columns={\"image_name\": \"id\"})\n",
    "train_crowd_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_crowd_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_crowd_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_crowd_file = Path(\"../../../mlops/crowdhuman/annotation_val_with_classes.csv\")\n",
    "val_crowd_df = read_df(val_crowd_file)\n",
    "val_crowd_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_crowd_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train_dataset import TrainDataset\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "train_crowd_dir = Path(data_crowd_dir, \"Images\")\n",
    "train_crowd_dataset = TrainDataset(train_crowd_dir, train_crowd_df, train_transform)\n",
    "val_crowd_dataset = TrainDataset(train_crowd_dir, val_crowd_df, val_transform)\n",
    "len(train_crowd_dataset), len(val_crowd_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_crowd_dataset.data_groups = train_crowd_dataset.data_groups[:1000]\n",
    "# val_crowd_dataset.data_groups = val_crowd_dataset.data_groups[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "image, centers, offsets, labels = train_crowd_dataset[44].values()\n",
    "for center, offset, label in zip(centers, offsets, labels):\n",
    "    x, y = map(int, center)\n",
    "    l, t, r, b = map(int, offset)\n",
    "    cv2.rectangle(\n",
    "        image,\n",
    "        (x - l, y - t),\n",
    "        (x + r, y + b),\n",
    "        (1.0, 0.0, 0.0) if label == 1 else (0.0, 0.0, 1.0) if label == 0 else (1.0, 1.0, 0.0),\n",
    "        2\n",
    "    )\n",
    "#     if not np.isnan(label):\n",
    "#         cv2.putText(\n",
    "#             image, LABELS[int(label)], (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 2, (1, 1, 1), 2\n",
    "#         )\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Khaliji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_khaliji_dir = Path(\"../../../mlops/khaliji\")\n",
    "!ls $data_khaliji_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_khaliji_file = Path(\"../../../mlops/khaliji/annotation_train_with_classes.csv\")\n",
    "train_khaliji_df = read_df(train_khaliji_file)\n",
    "train_khaliji_df = train_khaliji_df.rename(columns={\"image_name\": \"id\"})\n",
    "train_khaliji_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_khaliji_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train_dataset import TrainDataset\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "train_khaliji_dir = Path(data_khaliji_dir, \"Images\")\n",
    "train_khaliji_dataset = TrainDataset(train_khaliji_dir, train_khaliji_df, train_transform)\n",
    "len(train_khaliji_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "image, centers, offsets, labels = train_khaliji_dataset[1].values()\n",
    "for center, offset, label in zip(centers, offsets, labels):\n",
    "    x, y = map(int, center)\n",
    "    l, t, r, b = map(int, offset)\n",
    "    cv2.rectangle(\n",
    "        image,\n",
    "        (x - l, y - t),\n",
    "        (x + r, y + b),\n",
    "        (1.0, 0.0, 0.0) if label == 1 else (0.0, 0.0, 1.0) if label == 0 else (1.0, 1.0, 0.0),\n",
    "        2\n",
    "    )\n",
    "#     if not np.isnan(label):\n",
    "#         cv2.putText(\n",
    "#             image, LABELS[int(label)], (x, y - 10), cv2.FONT_HERSHEY_PLAIN, 2, (1, 1, 1), 2\n",
    "#         )\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorboard --logdir=log --host=0.0.0.0 --port=8099"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def seedEverything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p checkpoint log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from person_detection.centernet.src.pose_dla_dcn import get_pose_net as get_dla_dcn\n",
    "from src.trainer import Trainer\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "num_layers = 34\n",
    "heads = {\"hm\": 1, \"wh\": 4, \"reg\": 2, \"id\": 128, \"clss\": len(LABELS)}\n",
    "head_conv = 256\n",
    "net = get_dla_dcn(num_layers, heads, head_conv)\n",
    "# model_path = Path(\"/home/username/Downloads/crowdhuman_dla34.pth)\"\n",
    "model_path = Path(\"/home/username/Downloads/fairmot_dla34.pth\")\n",
    "# model_path = Path(\"checkpoint/14.pth\")\n",
    "trainer = Trainer(\n",
    "    net, image_size=IMAGE_SIZE, device=\"cuda\", checkpoint_dir=\"checkpoint\", log_dir=\"log\",\n",
    "    model_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.optimizer.param_groups[0][\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.ConcatDataset([train_crowd_dataset, train_khaliji_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seedEverything(2020)\n",
    "trainer.fit(train_dataset, val_crowd_dataset, batch_size=16, start_epoch=0, end_epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.optimizer.param_groups[0][\"lr\"] = 1e-5\n",
    "trainer.optimizer.param_groups[0][\"weight_decay\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seedEverything(2020)\n",
    "trainer.fit(train_dataset, val_crowd_dataset, batch_size=16, start_epoch=20, end_epoch=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    net, image_size=IMAGE_SIZE, device=\"cuda\", checkpoint_dir=\"checkpoint\", log_dir=\"log\",\n",
    "    model_path=\"checkpoint/18.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.eval(val_crowd_dataset, batch_size=16)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_null_df(data_file):\n",
    "    data_df = pd.read_csv(data_file)\n",
    "    data_df = data_df[(data_df[\"head_attr.ignore\"] != 1) & (data_df[\"head_attr.occ\"] != 1)]\n",
    "    data_df = data_df[data_df[\"gender\"].isnull() & data_df[\"age\"].isnull()]\n",
    "    data_df = data_df.reset_index(drop=True)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_crowd_file = Path(\"../../../mlops/crowdhuman/annotation_train_with_classes.csv\")\n",
    "train_null_df = read_null_df(train_crowd_file)\n",
    "train_null_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "# IMAGE_SIZE = (416, 416)\n",
    "IMAGE_SIZE = (1088, 608)\n",
    "# IMAGE_SIZE = (608, 608)\n",
    "\n",
    "test_transform = A.Compose(\n",
    "    [\n",
    "        AspectRatioResize(height=IMAGE_SIZE[1], width=IMAGE_SIZE[0], scale=(1, 1), p=1.0),\n",
    "        A.PadIfNeeded(\n",
    "            min_height=IMAGE_SIZE[1], min_width=IMAGE_SIZE[0], border_mode=cv2.BORDER_CONSTANT,\n",
    "            value=[128, 128, 128]\n",
    "        )\n",
    "#         A.Resize(height=IMAGE_SIZE[1], width=IMAGE_SIZE[0], p=1.0)\n",
    "    ],\n",
    "    p=1.0,\n",
    "    bbox_params=A.BboxParams(\n",
    "        format=\"pascal_voc\",\n",
    "        min_area=0,\n",
    "        min_visibility=0,\n",
    "        label_fields=[\"bboxes_id\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.test_dataset import TestDataset\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "train_dir = Path(data_dir, \"Images\")\n",
    "test_dataset = TestDataset(train_dir, train_null_df, test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataset), train_null_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    net, image_size=IMAGE_SIZE, device=\"cuda\", checkpoint_dir=\"checkpoint\",\n",
    "    model_path=\"checkpoint/10.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = trainer.predict(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape, train_null_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"pred\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_null_df = pd.merge(train_null_df, test_df, on=[\"id\", \"extra.box_id\"], how=\"inner\")\n",
    "new_train_null_df = new_train_null_df.loc[\n",
    "    (new_train_null_df[\"score\"] > 0.9) & (new_train_null_df[\"score\"] < 0.95),\n",
    "    [\"id\", \"hbox\", \"vbox\", \"fbox\", \"pred\"]\n",
    "]\n",
    "new_train_null_df = new_train_null_df.rename(columns={\"pred\": \"gender\"})\n",
    "# new_train_null_df = new_train_null_df.rename(columns={\"pred\": \"age\"})\n",
    "new_train_null_df = new_train_null_df.reset_index(drop=True)\n",
    "new_train_null_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_null_df[\"gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_null_df.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "train_null_dataset = TrainDataset(train_dir, new_train_null_df, train_transform)\n",
    "new_train_dataset = ConcatDataset([train_dataset, train_null_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = new_train_null_df[\"gender\"].value_counts() + train_crowd_df[\"gender\"].value_counts()\n",
    "nb = nb.sort_index().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.sum() / (nb * len(nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "trainer.criterion.criterion.weight = torch.as_tensor(\n",
    "    nb.sum() / (nb * len(nb)), dtype=torch.float32, device=trainer.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seedEverything(2020)\n",
    "trainer.fit(new_train_dataset, val_crowd_dataset, batch_size=16, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
