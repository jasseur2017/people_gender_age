{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "b8lmxhyKmtEp"
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"/home/jasseur/Downloads/videos\")\n",
    "!ls $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "IMAGE_SIZE = (1088, 608)\n",
    "\n",
    "val_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(height=IMAGE_SIZE[1], width=IMAGE_SIZE[0], p=1.0),\n",
    "    ],\n",
    "    p=1.0,\n",
    "    bbox_params=A.BboxParams(\n",
    "        format=\"pascal_voc\",\n",
    "        min_area=0,\n",
    "        min_visibility=0,\n",
    "        label_fields=[\"person_ids\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.test_dataset import TestDataset\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "test_dataset = TestDataset(data_dir, val_transform)\n",
    "test_dataset.data_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color(idx):\n",
    "    idx = float(idx * 3)\n",
    "    color = ((37 * idx) % 255, (17 * idx) % 255, (29 * idx) % 255)\n",
    "    return color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_dashed_rectangle(image, x1y1, x2y2, color, thickness=2, dash=8):\n",
    "    height, width, _ = image.shape\n",
    "    x1, y1 = x1y1\n",
    "    x2, y2 = x2y2\n",
    "    x1 = max(0, min(x1, width - 1))\n",
    "    x2 = max(x1, min(x2, width - 1))\n",
    "    y1 = max(0, min(y1, height - 1))\n",
    "    y2 = max(y1, min(y2, height - 1))\n",
    "    xs = [i for i in range(x1, x2 + 1) if (i % (2 * dash)) < dash]\n",
    "    ys = [i for i in range(y1, y2 + 1) if (i % (2 * dash)) < dash]\n",
    "    image[ys, x1 - thickness: x1 + thickness, :] = color\n",
    "    image[ys, x2 - thickness: x2 + thickness, :] = color\n",
    "    image[y1 - thickness: y1 + thickness, xs, :] = color\n",
    "    image[y2 - thickness: y2 + thickness, xs, :] = color\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k2yERNKZSbyV"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from person_detection.centernet.src.pose_dla_dcn import get_pose_net as get_dla_dcn\n",
    "from src.trainer import Trainer\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "num_layers = 34\n",
    "heads = {\"hm\": 1, \"wh\": 4, \"reg\": 2, \"id\": 128}\n",
    "head_conv = 256\n",
    "net = get_dla_dcn(num_layers, heads, head_conv)\n",
    "# model_path = Path(\"/home/jasseur/Downloads/crowdhuman_dla34.pth\")\n",
    "model_path = Path(\"/home/jasseur/Downloads/fairmot_dla34.pth\")\n",
    "trainer = Trainer(\n",
    "    net, image_size=IMAGE_SIZE, device=\"cuda\", checkpoint_dir=\"checkpoint\",\n",
    "    nID=1000, model_path=model_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENDERS = [\"M\", \"F\"]\n",
    "AGES = [\"1-2\", \"3-9\", \"10-20\", \"21-25\", \"26-27\", \"28-31\", \"32-36\", \"37-45\", \"46-54\", \"55-65\", \"66-116\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twMybZUEwBP5"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "from src.track import Track\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def write_video(video, output_path):\n",
    "    desired_fps = test_dataset.desired_fps\n",
    "    video_writer = cv2.VideoWriter(\n",
    "        str(output_path), cv2.VideoWriter_fourcc(*\"MP4V\"), # \"MJPG\"), \n",
    "        desired_fps, IMAGE_SIZE\n",
    "    )\n",
    "    try:\n",
    "        for frame, time, state_tracks in trainer.predict_video(video):\n",
    "            frame = (255 * frame).astype(np.uint8)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "            for track in state_tracks:\n",
    "                (x1, y1, x2, y2) = map(int, track.bbox)\n",
    "                color = get_color(track.id)\n",
    "                if track.state == Track.TrackState.TRACKED:\n",
    "                    cv2.rectangle(\n",
    "                        frame, (x1, y1), (x2, y2), color, 2, lineType=cv2.LINE_AA\n",
    "                    )\n",
    "                else:\n",
    "                    cv2.rectangle(\n",
    "                        frame, (x1, y1), (x2, y2), color, 1, lineType=cv2.LINE_AA\n",
    "                    )\n",
    "                # (sx1, sy1, sx2, sy2) = map(int, track.get_state_bbox())\n",
    "                # draw_dashed_rectangle(\n",
    "                #     frame, (sx1, sy1), (sx2, sy2), color, 2\n",
    "                # )\n",
    "                cv2.putText(\n",
    "                    frame, f\"{track.id}_{GENDERS[track.getGender()]}_{AGES[track.getAge()]}\", (x1, y1 + 30), cv2.FONT_HERSHEY_PLAIN,\n",
    "                    1, (0, 0, 255), 2\n",
    "                )\n",
    "            cv2.putText(\n",
    "                frame, \"%.2f\" % time, (10, 10), cv2.FONT_HERSHEY_PLAIN,\n",
    "                1, (0, 0, 255), 2\n",
    "            )\n",
    "            video_writer.write(frame)\n",
    "    finally:\n",
    "        video_writer.release()\n",
    "\n",
    "video_name, video = test_dataset[0]\n",
    "output_path = Path(\"test.mp4\")\n",
    "write_video(video, output_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3yWK5Ah9PqJS"
   },
   "source": [
    "## ONNX export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OxwVj-Vhr-cS"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "\n",
    "    def __init__(self, model, pre_detector):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.pre_detector = pre_detector\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, image):\n",
    "        preds = self.model(image)\n",
    "        preds_hm, preds_wh, preds_reg, preds_id = (\n",
    "            preds[0][\"hm\"], preds[0][\"wh\"], preds[0][\"reg\"], preds[0][\"id\"]\n",
    "        )\n",
    "        preds_hm = self.pre_detector(preds_hm)\n",
    "        return preds_hm, preds_wh, preds_reg, preds_id"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "PfYloeYwKf98"
   },
   "source": [
    "deform_conv2d\n",
    "- input: batch_size x in_channels x h x w\n",
    "- weight: out_channels x in_channels x kernel_size x kernel_size\n",
    "- offset: batch_size x (2 * kernel_size * kernel_size) x h x w\n",
    "- mask: batch_size x (kernel_size * kernel_size) x h x w\n",
    "- bias: out_channels\n",
    "\n",
    "- output = batch_size x out_channels x h_out x w_out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "yYxD8v3IBLti"
   },
   "source": [
    "https://github.com/onnx/tutorials/blob/master/PyTorchCustomOperator/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dLzLQr2VcDkz"
   },
   "outputs": [],
   "source": [
    "from torch.onnx.symbolic_helper import parse_args\n",
    "\n",
    "@parse_args('v', 'v', 'v', 'v', 'v', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i')\n",
    "def symbolic_deform_conv2d(\n",
    "    g, input, weight, offset, mask, bias,\n",
    "    stride_h, stride_w, pad_h, pad_w, dil_h, dil_w,\n",
    "    n_weight_grps, n_offset_grps, use_mask\n",
    "):\n",
    "    return g.op(\n",
    "        \"custom_domain::deform_conv2d\", input, weight, offset, mask, bias,\n",
    "        stride_h_i=stride_h, stride_w_i=stride_w, pad_h_i=pad_h, pad_w_i=pad_w,\n",
    "        dil_h_i=dil_h, dil_w_i=dil_w, out_channels_i=bias.type().sizes()[0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k10CDAh4YgXq"
   },
   "outputs": [],
   "source": [
    "from torch.onnx import register_custom_op_symbolic\n",
    "register_custom_op_symbolic(\"torchvision::deform_conv2d\", symbolic_deform_conv2d, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_qQ7hP7tH6I"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "name = \"fairmot_dla34\"\n",
    "output_names = [\"hm\", \"wh\", \"reg\", \"id\"]\n",
    "ONNX_FILE = f\"{name}.onnx\"\n",
    "\n",
    "my_model = MyModel(trainer.net, trainer.pre_detector)\n",
    "my_model.cuda()\n",
    "my_model.eval()\n",
    "x = torch.rand(1, 3, 416, 416, device=\"cuda\")\n",
    "np.save(f\"{name}_x.npy\", x.cpu().numpy())\n",
    "torch.onnx.export(\n",
    "    my_model, x, ONNX_FILE, verbose=False, opset_version=11,\n",
    "    custom_opsets={\"custom_domain\": 2},\n",
    "    output_names=output_names\n",
    ")\n",
    "with torch.no_grad():\n",
    "    y = my_model(x)\n",
    "for i, output_name in enumerate(output_names):\n",
    "    np.save(f\"{name}_y_{output_name}.npy\", y[i].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp $name_*.npy ../../../../iot/nvidia_jetson/tensorrt/test/onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp $name.onnx ../../../../iot/nvidia_jetson/tensorrt/onnx/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "b8lmxhyKmtEp"
   ],
   "name": "fairmot-test.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
